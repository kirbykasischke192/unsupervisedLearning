# https://stackabuse.com/k-means-clustering-with-scikit-learn/
# https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1
# https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation

import pandas as pd  
import numpy as np  
import matplotlib.pyplot as plt
from sklearn import metrics  
import time as time

satdata = pd.read_csv("./../data/sat_data.csv")
pendata = pd.read_csv("./../data/pen_data.csv")

Xsat = satdata.iloc[:, :-1].values
ysat = satdata.iloc[:, 36].values

Xpen = pendata.iloc[:, :-1].values
ypen = pendata.iloc[:, 16].values

from sklearn.model_selection import train_test_split  
X_train_sat, X_test_sat, y_train_sat, y_test_sat = train_test_split(Xsat, ysat, test_size=0.20)
X_train_pen, X_test_pen, y_train_pen, y_test_pen = train_test_split(Xpen, ypen, test_size=0.20)

from sklearn.preprocessing import StandardScaler  
pen_scaler = StandardScaler()
sat_scaler = StandardScaler()

#We normalize the values so the kmeans doesn't gravitate towards very high values
X_train_pen_og = pen_scaler.fit_transform(X_train_pen)
X_train_sat_og = sat_scaler.fit_transform(X_train_sat)
X_test_pen_og = pen_scaler.fit_transform(X_test_pen)
X_test_sat_og = sat_scaler.fit_transform(X_test_sat)

pen_time = 0
sat_time = 0

pen_accuracy = []
sat_accuracy = []
sat_homog = []
pen_homog = []
sat_comp = []
sat_comp_train = []
sat_comp_test = []
pen_comp = []
pen_comp_train = []
pen_comp_test = []
sat_v = []
pen_v = []

y_train_sat = [label - 1 for label in y_train_sat]
y_test_sat = [label - 1 for label in y_test_sat]
#=========================================================1. Uncomment to produce plots A, B, C=====================================
# for i in range(1, 21):

#     from sklearn.cluster import KMeans 
#     kmeans_sat = KMeans(n_clusters = i) # 7 categories, domain knowledge
#     kmeans_pen = KMeans(n_clusters = i) # 10 categories, domain knowledge
#     start_time = time.time()
#     kmeans_sat.fit(X_train_sat_og)
#     sat_labels = kmeans_sat.labels_
#     sat_labels_train = kmeans_sat.labels_
#     sat_labels_test = kmeans_sat.predict(X_test_sat_og)
#     end_time = time.time()
#     sat_time = end_time - start_time
#     start_time = time.time()
#     kmeans_pen.fit(X_train_pen_og)
#     pen_labels = kmeans_pen.labels_
#     pen_labels_train = kmeans_pen.labels_
#     pen_labels_test = kmeans_pen.predict(X_test_pen_og)
#     end_time = time.time()
#     pen_time = end_time - start_time
#================================================================================================================================
#================================================2. Uncomment to calculate homogeneity measures (Use plot C)========================
#     comp = metrics.homogeneity_score(sat_labels_train, y_train_sat)
#     sat_comp_train.append(comp)
#     comp = metrics.homogeneity_score(sat_labels_test, y_test_sat)
#     sat_comp_test.append(comp)
#     comp = metrics.homogeneity_score(pen_labels_train, y_train_pen)
#     pen_comp_train.append(comp)
#     comp = metrics.homogeneity_score(pen_labels_test, y_test_pen)
#     pen_comp_test.append(comp)

#==============3. Uncomment to view homogeneity, completeness, and v measure for satellite and digits data (Use plot B)================
    # homog, comp, v = metrics.homogeneity_completeness_v_measure(sat_labels, y_train_sat)
    # sat_homog.append(homog)
    # sat_comp.append(comp)
    # sat_v.append(v)

    # homog, comp, v = metrics.homogeneity_completeness_v_measure(pen_labels, y_train_pen)
    # pen_homog.append(homog)
    # pen_comp.append(comp)
    # pen_v.append(v)
#===================================================================================================================================
#======================4. Uncomment to permute predicted labels and compute accuracy (Use Plot A)=======================
    # pred_sat_labels = kmeans_sat.predict(X_test_sat)
    # pred_pen_labels = kmeans_pen.predict(X_test_pen)

#     from scipy.stats import mode
#     # match true labels with the labels generated by the algorithm so we can compare accuracy
#     sat_labels = np.zeros_like(pred_sat_labels)
#     for i in range(1,8):
#         mask = (pred_sat_labels == i)
#         sat_labels[mask] = mode(y_test_sat[mask])[0]

#     from sklearn.metrics import accuracy_score
#     sat_accuracy.append(accuracy_score(y_test_sat, sat_labels))

#     pen_labels = np.zeros_like(pred_pen_labels)
#     for i in range(10):
#         mask = (pred_pen_labels == i)
#         pen_labels[mask] = mode(y_test_pen[mask])[0]

#     pen_accuracy.append(accuracy_score(y_test_pen, pen_labels))

# sat_error = [1 - accuracy for accuracy in sat_accuracy]
# pen_error = [1 - accuracy for accuracy in pen_accuracy]
#===================================================================================================================================
#=======================================Plot A====================================================================================== 
# plt.figure(figsize=(12, 6))  
# plt.plot(range(10, 101), sat_error, label='Testing Error', color='red', linestyle='solid', marker='')
# plt.title('Error Rate vs Number of Iterations - KMeans (Satellite)')  
# plt.xlabel('Number of Iterations')  
# plt.ylabel('Error')
# plt.show()
# plt.figure(figsize=(12, 6))
# plt.plot(range(10, 101), pen_error, label='Testing Error', color='red', linestyle='solid', marker='')
# plt.title('Error Rate vs Number of Iterations - KMeans (Digits)')  
# plt.xlabel('Number of Iterations')  
# plt.ylabel('Error')
# plt.show()
#===================================================================================================================================
#===================================Plot B==========================================================================================
# plt.figure(figsize=(12, 6))  
# plt.plot(range(1, 21), sat_comp, label='Completeness', color='red', linestyle='solid', marker='')
# plt.plot(range(1, 21), sat_homog, label='Homogeneity', color='green', linestyle='solid', marker='')
# plt.plot(range(1, 21), sat_v, label='V-Measure', color='blue', linestyle='solid', marker='')
# plt.title('Homogeneity, Completeness, and V Measure vs. Number of Components - KMeans (Satellite)')  
# plt.xlabel('Number of Components')  
# plt.ylabel('Score')
# plt.legend()
# plt.show()
# plt.figure(figsize=(12, 6))
# plt.plot(range(1, 21), pen_comp, label='Completeness', color='red', linestyle='solid', marker='')
# plt.plot(range(1, 21), pen_homog, label='Homogeneity', color='green', linestyle='solid', marker='')
# plt.plot(range(1, 21), pen_v, label='V-Measure', color='blue', linestyle='solid', marker='')
# plt.title('Homogeneity, Completeness, and V Measure vs. Number of Components - KMeans (Digits)')  
# plt.xlabel('Number of Components')  
# plt.ylabel('Score')
# plt.legend()
# plt.show()
#==================================================================================================================================

# =======Plot C: Use to show completeness in the testing and training data=========================================================
# plt.figure(figsize=(12, 6))  
# plt.plot(range(1, 21), sat_comp_train, label='Train', color='red', linestyle='solid', marker='')
# plt.plot(range(1, 21), sat_comp_test, label='Test', color='green', linestyle='solid', marker='')
# plt.title('Homogeneity (Test and Train) vs. Number of Components - KMeans (Satellite)')  
# plt.xlabel('Number of Components')  
# plt.ylabel('Homogeneity')
# plt.legend()
# plt.show()
# plt.figure(figsize=(12, 6))
# plt.plot(range(1, 21), pen_comp_train, label='Train', color='red', linestyle='solid', marker='')
# plt.plot(range(1, 21), pen_comp_test, label='Test', color='green', linestyle='solid', marker='')
# plt.title('Homogeneity (Test and Train) vs. Number of Components - KMeans (Digits)')  
# plt.xlabel('Number of Components')  
# plt.ylabel('Homogeneity')
# plt.legend()
# plt.show()
#=====================================================================================================================
#==============================5. Uncomment to examine the homogeneity in Kmeans using different feature selection algorithms=====================
# algs = ['PCA','ICA','Random Projections','Feature Agglomeration']

# sat_homog = []
# pen_homog = []

# from sklearn.decomposition import PCA
# sat_pca = PCA(n_components=20)
# pen_pca = PCA(n_components=10)  
# X_train_sat = sat_pca.fit_transform(X_train_sat_og)   
# X_train_pen = pen_pca.fit_transform(X_train_pen_og)

# from sklearn.cluster import KMeans
# kmeans_sat = KMeans(n_clusters = 6)
# kmeans_pen = KMeans(n_clusters = 11)
# sat_labels = kmeans_sat.fit_predict(X_train_sat)
# pen_labels = kmeans_pen.fit_predict(X_train_pen)
# sat_homog.append(metrics.homogeneity_score(sat_labels, y_train_sat))
# pen_homog.append(metrics.homogeneity_score(pen_labels, y_train_pen))

# from sklearn.decomposition import FastICA
# sat_ica = FastICA(n_components=20)
# pen_ica = FastICA(n_components=10)  
# X_train_sat = sat_ica.fit_transform(X_train_sat_og)   
# X_train_pen = pen_ica.fit_transform(X_train_pen_og)


# kmeans_sat = KMeans(n_clusters = 6)
# kmeans_pen = KMeans(n_clusters = 11)
# sat_labels = kmeans_sat.fit_predict(X_train_sat)
# pen_labels = kmeans_pen.fit_predict(X_train_pen)
# sat_homog.append(metrics.homogeneity_score(sat_labels, y_train_sat))
# pen_homog.append(metrics.homogeneity_score(pen_labels, y_train_pen))

# from sklearn.random_projection import GaussianRandomProjection
# sat_rp = GaussianRandomProjection(n_components=20)
# pen_rp = GaussianRandomProjection(n_components=10)  
# X_train_sat = sat_rp.fit_transform(X_train_sat_og)   
# X_train_pen = pen_rp.fit_transform(X_train_pen_og)


# kmeans_sat = KMeans(n_clusters = 6)
# kmeans_pen = KMeans(n_clusters = 11)
# sat_labels = kmeans_sat.fit_predict(X_train_sat)
# pen_labels = kmeans_pen.fit_predict(X_train_pen)
# sat_homog.append(metrics.homogeneity_score(sat_labels, y_train_sat))
# pen_homog.append(metrics.homogeneity_score(pen_labels, y_train_pen))

# from sklearn.cluster import FeatureAgglomeration
# sat_fa = FeatureAgglomeration(n_clusters=20)
# pen_fa = FeatureAgglomeration(n_clusters=10)  
# X_train_sat = sat_fa.fit_transform(X_train_sat_og)   
# X_train_pen = pen_fa.fit_transform(X_train_pen_og)

# kmeans_sat = KMeans(n_clusters = 6)
# kmeans_pen = KMeans(n_clusters = 11)
# sat_labels = kmeans_sat.fit_predict(X_train_sat)
# pen_labels = kmeans_pen.fit_predict(X_train_pen)
# sat_homog.append(metrics.homogeneity_score(sat_labels, y_train_sat))
# pen_homog.append(metrics.homogeneity_score(pen_labels, y_train_pen))


# plt.figure(figsize=(12, 6))  
# plt.bar(algs, sat_homog)
# plt.title('Homogeneity vs. Dimensionality Reduction Algorithm - KMeans (Satellite)')  
# plt.xlabel('Dimensionality Reduction Algorithm')  
# plt.ylabel('Homogeneity')
# plt.legend()
# plt.show()
# plt.figure(figsize=(12, 6)) 
# plt.bar(algs, pen_homog)
# plt.title('Homogeneity vs. Dimensionality Reduction Algorithm - KMeans (Digits)')  
# plt.xlabel('Dimensionality Reduction Algorithm')  
# plt.ylabel('Homogeneity')
# plt.legend()
# plt.show()
#============================================================================================================================================
#====6. Uncomment to view how the number of components in feature selection effects the cluster homogeneity for each feature selection algorithm=====
sat_homog_alg = []
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
kmeans_sat = KMeans(n_clusters = 6)
for i in range(2,37):
    sat_pca = PCA(n_components=i)
    sat_labels = kmeans_sat.fit_predict(X_train_sat_og)
    sat_homog.append(metrics.homogeneity_score(sat_labels, y_train_sat))
    
    X_train_sat = sat_pca.fit_transform(X_train_sat_og)
    sat_labels = kmeans_sat.fit_predict(X_train_sat)
    sat_homog_alg.append(metrics.homogeneity_score(sat_labels, y_train_sat))

pen_homog_alg = [] 
kmeans_pen = KMeans(n_clusters = 11)
for i in range(2, 17):  
    pen_pca = PCA(n_components=i)  
    pen_labels = kmeans_pen.fit_predict(X_train_pen_og)
    pen_homog.append(metrics.homogeneity_score(pen_labels, y_train_pen))

    X_train_pen = pen_pca.fit_transform(X_train_pen_og)
    pen_labels = kmeans_pen.fit_predict(X_train_pen)
    pen_homog_alg.append(metrics.homogeneity_score(pen_labels, y_train_pen))

plt.figure(figsize=(12, 6))  
plt.plot(range(2, 37), sat_homog, label='No PCA', color='red', linestyle='solid', marker='')
plt.plot(range(2, 37), sat_homog_alg, label='w/ PCA', color='green', linestyle='solid', marker='')
plt.title('Homogeneity vs. Number of Features - KMeans (Satellite)')  
plt.xlabel('Number of Features')  
plt.ylabel('Homogeneity')
plt.legend()
plt.show()
plt.figure(figsize=(12, 6)) 
plt.plot(range(2, 17), pen_homog, label='No PCA', color='red', linestyle='solid', marker='')
plt.plot(range(2, 17), pen_homog_alg, label='w/ PCA', color='green', linestyle='solid', marker='')
plt.title('Homogeneity vs. Number of Features - KMeans (Digits)')  
plt.xlabel('Number of Features')  
plt.ylabel('Homogeneity')
plt.legend()
plt.show()
#==================================================================================================================================================